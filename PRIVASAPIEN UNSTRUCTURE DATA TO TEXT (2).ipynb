{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce8747d",
   "metadata": {},
   "source": [
    "# RESUME PDF , WORD TO EXTRACT TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63c00861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leowa\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\leowa\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Word document: Aarthi Data Engineer.docx\n",
      "Processing PDF: Avinash Kormatha.pdf\n",
      "Filename: Aarthi Data Engineer.docx\n",
      "Chunk: \n",
      "\n",
      "Chunk: AARTHI AWASTHI aarthi211097gmailcom 316 530 1525 SUMMARY Over 7 years of IT experience in Design Development Maintenance and Support of Big Data Applications Experience in Data Engineering Data Pipeline Design Development and Implementation as a Data EngineerData Developer and Data Modeler Optimized data queries and data processing tasks within Azure Synapse Analytics and Azure Data Factory improving performance and efficiency Experience on Migrating SQL database to Azure Data Lake Azure Data Lake Analytics Azure SQL Database Azure Data Bricks and Azure SQL Data Warehouse and controlling and granting database access and Migrating Onpremises databases to Azure Data Lake store using Azure Data Factory Strong experience in Software Development Life Cycle SDLC including Requirements Analysis Design Specification and Testing as per Cycle in both Waterfall and Agile methodologies Strong experience in writing scripts using Python API PySpark API and Spark API for analyzing data Python Libraries PySpark Pytest Pymongo PyExcel Psycopg NumPy and Pandas Hands On experience on Spark Core Spark SQL Spark Streaming and creating Data Frames handle in SPARK with Scala Experience in NoSQL databases and worked on table row key design and to load and retrieve data for realtime data processing and performance improvements based on data access patterns Extensive experience in Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MapReduce concepts Expertise in LLM models with ChatGPT and OpenAI Experience in building largescale highly available Web Applications Working knowledge of web services and other integration patterns Developed Simple to complex MapReduce and Streaming jobs using Java and Scala language Developed Hive scripts for end useranalyst requirements to perform ad hoc analysis with Hive to handle less important bulk ETL jobs Handson use of Spark and Scala API to compare the performance of Spark with Hive and SQL and Spark SQL to manipulate Data Frames in Scala Expertise in Python and Scala userdefined functions UDF for Hive and Pig using Python Experience in developing MapReduce Programs using Apache Hadoop for analyzing the big data as per the requirement Handson Spark MLlib utilities such as classification regression clustering collaborative filtering dimensionality reduction Handson experience in developing and deploying enterprisebased applications using major Hadoop ecosystem components like MapReduce YARN Hive HBase Flume Sqoop Spark MLlib Spark GraphX Spark SQL Kafka Experience in application of various data sources like Oracle SE2 SQL Server Flat Files and Unstructured files into a data warehouse Able to use Sqoop to migrate data between RDBMS NoSQL databases and HDFS Experience in Extraction Transformation and Loading ETL data from various sources into Data Warehouses as well as data processing like collecting aggregating and moving data from various sources using Apache Flume Kafka PowerBI and Microsoft SSIS Skilled in System Analysis ERDimensional Data Modeling Database Design and implementing RDBMS specific features Knowledge of working with Proof of Concepts PoCs and gap analysis and gathered necessary data for analysis from different sources prepared data for data exploration using data munging and Teradata Well experienced in Normalization and Denormalization techniques for optimum performance in relational and dimensional database environments Worked extensively with integrating Druid into big data pipelines leveraging tools like Apache Kafka and Hadoop for seamless data flow and transformation Focused on optimizing Druid cluster configurations tuning indexing and segment loading to improve query performance and data retrieval times Experience includes building interactive dashboards that leverage Druids fast querying capabilities providing realtime insights and reporting for businesscritical applications Experience in developing customized UDFs in Python to extend Hive and Pig Latin functionality Good working experience on AWS infrastructure services Amazon S3 EMR Lambda functions and Amazon EC2 Understanding of snowflake cloud technology with snowflake utilities Snow SQL Snow pipe and bigdata model  TECHNICAL SKILLS Data Modeling Tools Erwin Data Modeler ER Studio v17 Programming Languages SQL PLSQL UNIX Mongo DB Data wave Python R SQL Server PostgreSQL database Methodologies RAD JAD System Development Life Cycle SDLC Agile Cloud Platform AWS Azure cloud Google Cloud Azure data factory Databases Oracle 12c11g Teradata R15R14 OLAP Tools Tableau SSAS SSIS Business Objects and Crystal Reports 9 ETLData warehouse Tools Informatica 9691 Tableau Splunk IntelliJ and New Relic Data Visualization Tools Tableau Power BI SAS Excel ETL   Operating System Windows Unix Sun Solaris Mac OS Big Data Tools Hadoop Ecosystem Map Reduce  EDUCATION Bachelors in commerce from Osmania university 2015  EXPERIENCE Walmart AR   Sr Data Engineer                                                                                                                             May 2023 to Present Responsibilities  Analyze and cleanse raw data using HiveQL Proficient in using Azure services such as Azure Data Lake Azure SQL Database Azure Synapse Analytics and Azure Cosmos DB for scalable data storage processing and analytics Experience in data transformations using MapReduce Hive for different file formats Analyze the user needs interact with various SORs to understand their incoming data structure and run POCs with the best possible processing framework in big data platform Worked with Spark for improving performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Utilized Microsoft Azure services such as Azure Data Factory Azure Blob Storage and Azure SQL Database to support data operations and analytics Developed UNIX shell scripts to load large number of files into HDFS from Linux File System Played a key role in Finalizing the tech stack for our project GPC and ran endtoend vigorous testing qualifying the user needs as well as tech requirements Experience with Apache Druid utilizing its capabilities to ingest and analyze large volumes of event data in realtime Developed and maintained scalable architectures using Druid for handling large datasets with high query performance ensuring efficient resource management and lowlatency querying Ran data formatting scripts in Python and created terabyte CSV files to be consumed by Hadoop MapReduce jobs Expertise in Snowflake to create and Maintain Tables and views Performed Kafka analysis feature selection feature extraction using Apache Spark Machine Learning streaming libraries in Python Skilled in creating and managing tables views indexes and stored procedures in SQL Server Used Pandas Numpy Scipy Scikitlearn NLTK in Python for scientific computing and data analysis Experienced in using Pandas Numpy SciPy Scikitlearn to develop various machine learning algorithms Developed Python code using version control tools like GIT and SVN on Vagrant machines Collaborated with intra applications teams to fit our business models on existing onprem platform setup Experience in creating tables dropping and altering at runtime without blocking updates and queries using HBase and Hive Worked on importing and exporting data from Snowflake Oracle and DB2 into HDFS and Hive using Sqoop for analysis visualization and to generate reports Encoded and decoded JSON objects using PySpark to create and modify the data frames in Apache Spark Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Experienced with popular NLP libraries such as NLTK SpaCy Hugging Face Transformers and Gensim leveraging them for various text processing and analysis tasks  Designed and implemented scalable ETL pipelines using Azure Data Factory integrating data from various sources and ensuring smooth data flow across Walmarts systems Strong PySpark experience essential for developing and optimizing data pipelines and transformations Extensive Python experience showcasing proficiency in core Python programming and scripting Demonstrated expertise in using Azure Databricks for data engineering tasks Handson experience with Azure Data Factory and Azure Data Lake for orchestrating and managing data workflows Proven ability to build reusable frameworks libraries and data platform capabilities Experience with REST API and GraphQL APIs highlighting knowledge of API integration and management Experience with Data Quality DQ capabilities focusing on building reusable DQ frameworks Ability to work independently while demonstrating system design thinking Used CloudWatch to monitor logs and log metrics generated by applications Integrated with Restful APIs to create ServiceNow Incidents when there is a process failure within the batch job Analyzed the SQL scripts and designed the solution to implement using PySpark Developed a capability to implement audit logging at required stages while applying business logic Implemented Spark Data Frames on huge incoming data sets of various data formats like JSON CSV Parquet Actively worked in resolving many of the Tech challenges One of them is handling the nested JSON with multiple data sections in the same file and converting them into Spark friendly data frames Reformatted the end results to SORs requested formats Environment Spark AWS Python Pandas sy HiveQL MySQL SOAP Snowflake NIFI Cassandra Spark SQL PySpark Cloudera HDFS Hive Apache Kafka Sqoop Scala Shell scripting Linux MySQL Oracle Enterprise DB Flink Modern data Five Tran Jenkins Eclipse Oracle Git  Charter Communications Negaunee MI                                                                                   Aug 2021 to Dec 2022 Data Engineer Responsibilities  Performed data extraction transformation loading and integration in data warehouse operational data stores and master data management Developed Spark Applications by using Scala and Implemented Apache Spark data processing project to handle data from various RDBMS and Streaming sources Data ingestion to one or more Azure services Azure Data Lake Azure Storage Azure SQL Azure DW and processing the data in Azure Databricks Implemented Apache Airflow for authoring scheduling and monitoring Data Pipelines Developed Mappings using Transformations like Expression Filter Joiner and Lookups for better data messaging and to migrate clean and consistent data Worked on setting up high availability for major production clusters and designed automatic failover control using Zookeeper and quorum journal nodes Designed several DAGs Directed Acyclic Graph for automating ETL pipelines Provide troubleshooting and best practices methodology for development teams Produce unit tests for Spark transformations and helper methods Design data processing pipelines Multiple batch jobs were written for processing hourly and daily data received through multiple sources like Adobe NoSQL databases Leveraged cloud and Azure computing technologies for automated machine learning and analytics pipelines Designed and implemented configurable data delivery pipeline for scheduled updates to customer facing data stores built with Python Involved in building the ETL source to Target mapping to load data into Data warehouse Developed a batch data ingestion pipeline using Sqoop and Hive to ingest transform and analyze Supply Chain data Use ObjectOriented Programming concepts to build Hive UDFs in Python that could be reused across the pipeline Extensively working on Hive queries to load data from various sources like Teradata DB2 Oracle Mainframe etc Extensively working on developing YAML files which are passed to the internal framework scripts having all the required JARs to move data Migrating the transformed data to Azure data lake to be consumed by consumers depending on the business need Creating tables in Hive in ORC format using correct compression ensuring no data loss Worked on performance tuning of the queries costbased optimizations to improve the performance and latency of the queries Writing shell scripts to automate the migration process and validate the data after loading Created data validation application in Scala for validating the data of source and target Working on performance tuning of the code with help of Hive dynamic partitioning set parameters mapper and reducer tuning Coordinate with Quality Assurance teams to ensure the developed code is accurately transformed into a working product Participate in daily meetings and standups to provide updates and project progress to the team Analyzed the system for new enhancementsfunctionalities and perform Impact analysis of the application for implementing ETL changes Implemented a Continuous Delivery pipeline with Docker and GitHub Azure Built performant scalable ETL processes to load cleanse and validate data Preparing associated documentation for specifications requirements and testing Environment Azure Data Factory USQL Azure Data Lake Analytics Azure SQL Azure DW Databricks GitHub Docker Talend Big Data Integration Snowflake Oracle SQL Server MySQL NoSQL MongoDB HBase Cassandra PythonPySpark Pytest Pymongo PyExcel Psycopg Matplotlib NumPy and Pandas  IBing Software Solutions Private Limited Hyderabad India                                                      Sep 2018 to Jul 2021 Data Engineer Responsibilities  Gather business requirements definition and design of the data sourcing and worked with the data warehouse architect on the development of logical data models Created sophisticated visualizations calculated columns and custom expressions and developed Map Chart Cross table Bar chart Tree map and complex reports which involves Property Controls Custom Expressions Investigated market sizing competitive analysis and positioning for product feasibility Worked on Business forecasting segmentation analysis and Data mining Extensively used Agile methodology as the Organization Standard to implement the data Models Used Micro service architecturebased services interacting through a combination of REST and Apache Kafka message brokers Created several types of data visualizations using Python  Matplotlib and Tableau Extracted data using SQL Queries to create reports Performed reverse engineering using Erwin to redefine entities attributes and relationships existing database Analyze functional and nonfunctional business requirements and translate into technical data requirements and create or update existing logical and physical data models Developed a data pipeline using Kafka to store data into HDFS Performed Regression testing for Golden Test Cases from State end to end test cases and automated the process using Python scripts Developed Spark jobs using Scala for faster realtime analytics and used Spark SQL for querying Generated graphs and reports using the ggplot package in RStudio for analytical models Developed and implemented R and Shiny application which showcases machine learning for business forecasting Developed predictive models using Decision Tree Random Forest and Naïve Bayes Used pandas NumPy seaborn SciPy Matplotlib Scikitlearn NLTK in Python for developing various machine learning algorithms Expertise in R Matlab Python and respective libraries Research on Reinforcement Learning and control TensorFlow Torch and machine learning model Scikitlearn Performed Kmeans clustering Regression and Decision Trees in R Worked on data cleaning and reshaping generated segmented subsets using NumPy and Pandas in Python Implemented various statistical techniques to manipulate the data like missing data imputation principal component analysis and sampling Responsible for design and development of Python programsscripts to prepare transform and harmonize data sets in preparation for modeling Worked with Market Mix Modeling to strategize the advertisement investments to better balance the ROI on advertisements Implemented clustering techniques like DBSCAN Kmeans Kmeans and Hierarchical clustering for customer profiling to design insurance plans according to their behavior pattern Used Grid Search to evaluate the best hyperparameters for my model and Kfold cross validation technique to train my model for best results Used Python 3X NumPy SciPy pandas scikitlearn seaborn and Spark 20 PySpark MLlib to develop a variety of models and algorithms for analytic purposes Performed Data Cleaning features scaling features engineering using pandas and NumPy packages in Python and build models using deep learning frameworks Implemented Univariate Bivariate and Multivariate Analysis on the cleaned data for getting actionable insights on the 500product sales data by using visualization techniques in Matplotlib Seaborn Bokeh and created reports in Power BI Processed the image data through the Hadoop distributed system by using Map and Reduce then stored into HDFS Created Session Beans and controller Servlets for handling HTTP requests from Talend Performed Data Visualization and Designed Dashboards with Tableau and generated complex reports including charts summaries and graphs to interpret the findings to the team and stakeholders Wrote documentation for each report including purpose data source column mapping transformation and user group Utilized Waterfall methodology for team and project management Used Git for version control with the Data Engineer team and Data Scientists colleagues Environment Spark YARN HIVE Pig Scala Mahout NiFi TDD Python Hadoop Azure DynamoDB Kibana NoSQL Sqoop MySQL  Maisa Solutions Private Limited Hyderabad India                                                                   Jun 2016 to Aug 2018 Hadoop developer Responsibilities Requirement discussions design the solution Responsible for choosing the Hadoop components Hive Pig MapReduce Sqoop Flume etc Responsible for building scalable distributed data solutions using Hadoop Hadoop cluster building and ingestion of data using Sqoop Imported streaming logs to HDFS through Flume Used Flume to collect aggregate and store the web log data from different sources like web servers mobile and network devices and pushed to HDFS Developed Use cases and technical prototyping for implementing Hive and Pig Worked in analyzing data using Hive Pig and custom MapReduce programs in Java Implemented partitioning dynamic partitions and buckets in Hive Installed and configured Hive Sqoop Flume Oozie on the Hadoop cluster Involved in scheduling Oozie workflow engine to run multiple Hive and Pig jobs Tuned the Hadoop Clusters and Monitored for the memory management and for the MapReduce jobs Responsible for Cluster maintenance Adding and removing cluster nodes Cluster Monitoring and Troubleshooting Developed a custom Framework capable of solving small files problem in Hadoop Deployed and administered 70 node Hadoop clusters Administered two smaller clusters Environment MapReduce HBase HDFS Hive Pig Java SQL Cloudera Manager Sqoop Flume Oozie Java JDK 16 Eclipse\n",
      "\n",
      "Filename: Avinash Kormatha.pdf\n",
      "Chunk: \n",
      "\n",
      "Chunk: Avinash KormathaPhone 512 8511476Email avinashkormathagmailcomPROFESSIONAL SUMMARY Over 6 years of experience in design development and analysis of Python Django and clientservertechnologiesbased applications Good experience in various phases of SDLC Requirement Analysis Design Development and Testing onvarious Development and Enhancement Projects Hands on experience in Agile Methodologies Scrum stories and sprints experience in a Python basedenvironment Experience in Object Oriented Design and Programming concepts using Python 3x and Java Experience working on several Standard Python Packages like NumPy Pandas Matplotlib PySide SciPywxPython PyTables etc Knowledge about setting up Python REST API Framework using Django Experience working with Python ORM Libraries including Django ORM Experience in implementing Model View Control MVC architecture using serverside applications likeDjango Flask and Pyramid for developing web applications Good experience in error and exceptional handling Strong experience in designing the automation framework using Shell scripting Experience in writing test plans test cases test specifications and test coverage Good experience in Testing framework by adding some helper classes and methods Good knowledge in implementation of Python best Practices PEP8 Experience in designing the automation framework using Perl and Shell scripting Experience in developed tested and documented initial release of Honeycombs Open Telemetry distributionfor Java while embedded in the integrations team Extensive experience in creating the Automation scripts using Python for testing applications as well as theintegration of these application APIs UIs based on REST calls using Python phrasing the JSON responses Experience in HTMLHTML5 DHTML CSSCSS3 JavaScript XML JSON Oracle PLSQL and Postgres Experience in writing Subqueries Stored Procedures Triggers Cursors and Functions on MySQL andPostgreSQL database Expertise in performing research to explore and identify new technological platforms Expertise in evangelizing open telemetrydistributed tracing both internally and externally Experience in creating Data Layer in MySQL Experience on extracted and loaded data using Python scripts and PLSQL packages Experience in migrating applications to the AWS cloud and involved in DevOps processes for build and deploysystemsTECHNICAL SKILLSLanguages PythonFrameworks DjangoPython Packages NumPy Pandas Matplotlib PySide SciPy wxPython PyTablesCloud Services AWS VMware Microsoft AzureWeb Technologies CSS HTML Bootstrap jQuery JavaScript Angular JSONWebApp Server Nginx Apache Tomcat IISIDE PyCharm PLSQL Developer TOADOther Tools Jenkins IBM Integration and Web Builder JIRA Bugzilla Rally Version OneManagement Tools SVN GitDatabases Oracle 1011g MySQL SQL ServerOperating Systems Unix Linux Windows and Mac OSEDUCATION  Master of Science in CS University of Texas at Arlington Arlington TX Bachelor of Technology in CSE GITAM University Hyderabad IndiaPROFESSIONAL EXPERIENCE Kroger Blue Ash OH Mar 2023  PresentPython Developer Evaluating business requirements and preparing detailed specifications that follow project guidelinesrequired to develop written programs Working on various phases of Software Development Life Cycle using Agile  Scrum Software developmentmethodology Fixing bugs enhancing applications by improving code reuse and upgraded performance by making effectiveuse of various design patterns Developed tested and documented initial release of Honeycombs Open Telemetry distribution for Javawhile embedded in the integrations team Evangelizing open telemetrydistributed tracing both internally and externally Building a distributed system for triggering and executing daily data processing jobs which contains a highavailability scheduler built with Python a cluster of workers built with Python and UI built with Pythonand Django Designed and developed highly scable distributed tracing pipeline leveraging open source opentelemetryspecification Responsible for the development of entire backend modules using Python Integrating Amazon Cloud Watch with Amazon EC2 instances for monitoring the log files and track metrics Creating AWS S3 buckets performed folder management in each bucket managing cloud trail logs andobjects within each bucket Used Apache Couchdb NoSQL in AWS Linux instance in parallel to RDS MySQL to store and analyze jobmarket info Creating Highly Available Environments using AutoScaling Load Balancers and SQS Automating the existing scripts for performance calculations using NumPy and SQL alchemy Defining branching labelling and merge strategies for all applications in Git Configuring Elastic Load Balancers with EC2 Auto Scaling groups Configuring S3 to host Static Web content Responsible for S3 Versioning and lifecycle policies to and backup files and archive files in Glacier Creating monitors alarms and notifications for EC2 hosts using CloudWatch Working on Performance Tuning and Query Optimization in AWS Redshift Designing application on AWS taking advantage of Disaster recovery Developing Cloud Formation scripts to build on demand EC2 instance formation Utilizing AWS CLI to automate backups of ephemeral datastores to S3 buckets and EBS Creating nightly AMIs for mission critical production servers as backups Configuring and maintaining the monitoring and alerting of production and corporate serversstorage usingCloudWatchEnvironment Python Django NumPy HTMLCSS MySQL GIT CICD GitHub AWS RDS IAM S3 Cloud WatchJPMC Plano TX Mar 2022  Feb 2023Python Developer Developed application logic using Python JavaScript Java Designed front end and backend of the application utilizing Python on Django Web Framework Configured AWS Multi Factor Authentication in IAM to implement 2 step authentication of users access usingGoogle Authenticator and AWS Virtual MFA Included security groups network ACLs Internet Gateways and Elastic IPs to ensure a safe area fororganization in AWS public cloud Used Python modules like Restful Matplotlib and Pandas library for statistical analysis and generatingcomplex graphical data and NumPy for numerical analysis  Written UNIX shell scripts to automate the jobs and scheduling Cron jobs for job automation using commandswith Crontab Wrote Ansible Playbooks with Python SSH as the Wrapper to Manage Configurations of AWS Nodes and TestPlaybooks on AWS instances using Python Designed AWS Cloud Formation templates to create custom sized VPC subnets NAT to ensure successfuldeployment of Web applications and database templates Created scripts in Python which integrated with Amazon API to control instance operations Coordinatedassisted developers with establishing and applying appropriate branching labelling namingconventions using GIT source control Developed Merge jobs in Python to extract and load data into MySQL database Built numerous Lambda functions using python and automated the process using the event created Involved in designing and developing Amazon EC2 Amazon S3 Amazon RDS Amazon Elastic Load BalancingAmazon SQS and other services of the AWS infrastructure Ran build jobs and integration tests on Jenkins MasterSlave configuration Managed Servers on the Amazon Web Services AWS platform instances Built servers using AWS importing volumes launching EC2 RDS creating security groups autoscaling loadbalancers ELBs in the defined virtual private connection Maintained the reliability availability and performance of Amazon Elastic Compute Cloud Amazon EC2instances Good understanding of NoSQL databases such as MongoDB Datastax Redis and Apache Cassandra Worked as part of a team to design and develop cloud data solutions Conducted systems design feasibility and cost studies and recommend costeffective cloud solutions such asAmazon Web Services AWS Monitored the AWS resources using Cloud Watch and application resources using Nagios Involved in complete SDLC life cycle  Designing Coding Testing Debugging and Production Support Involved in daytoday maintenance and problem analysis on storageEnvironment Python Django Matplotlib Pandas MySQL Linux HTML XHTML SVN CSS AJAX BugzillaJavaScript Apache Web ServerAmerican Automobile Association Phoenix AZ Oct 2020  Dec 2021Python Developer Responsible for gathering requirements system analysis design development testing and deployment Built DevOps Pipeline from the scratch using AWS Resources with Blue Green and Red Black Deploymentenabled Developed user interface using CSS HTML and ReactJS with Redux framework Created reports and dashboards by using Tableau 9x to communicate data insights significant featuresmodels score and the performance of new recommendation system to both technical and business teams Designed Forms Views Models using Djangos MVC software architecture pattern Used Python and Pandas library for data cleaning and aggregation Created RESTful APIs using Django Used Django Database APIs to successfully create database objects Used Ansible to configure and manage the infrastructure Wrote Python modules to extractload asset data from the MySQL source database Designed and developed UseCases Classes and Object Diagrams using UML Rational Rose for ObjectOriented Design techniques Designed and implemented a dedicated MySQL database server to drive the web apps and report on dailyprogress Used PyUnit the Python unit test framework for all Python applications Involved in developing the REST Web services to expose the business methods to external services in theproject Worked on front end frameworks like CSS Bootstrap for development of Web applications Participated in requirement gathering and worked closely with the architect in designing and modelling Developed tested and debugged software tools for clients and internal customers of the organization Coded test programs and evaluated existing engineering processes Created a Git repository and added the project to GitHub Environment Python Flask MongoDB JSON jQuery Elastic search XML PyMongo AWSKPIT Cummins India July 2019  Sep 2020Python Developer Implemented ETL as a code solution with Python using modules like SQL Alchemy and automated adminactivities to be taken care of from schemarelated changes to be taken care of from the deployment pipeline Responsible for the development of entire frontend and backend modules using Python Developed RESTful services using Django Wrote Python routines to log into the websites and fetch data for selected options Performed testing using Djangos Test Module Used Pandas NumPy Matplotlib SciPy in Python Designed and developed ETL integration patterns using Python on Spark Worked on specification of platforms interfaces and development tools Used Python scripts for automation of production tasks Generated property list for every application dynamically using Python Rewrote existing Java application in Python module to deliver a certain format of data Analysed various crossfunctional multiplatform applications systems enforcing Python best practices andprovided guidance in making long term scalable architectural design decisions Involved in various phases of Software Development Life Cycle SDLC such as requirements gatheringmodelling analysis design and development Used PyQuery for selecting DOM elements when parsing HTML Created PyUnit test scripts and used them for unit testing Worked with JSON based REST Web servicesEnvironment Python Django Pandas NumPy Matplotlib SciPy SQL Alchemy Spark PyQuery PyUnit RestfulJSON SDLCUnilever India May 2018  Jun 2019Python Developer Worked on development of application architecture and blueprints to define application components Designed and developed the user interface of the project with HTML CSS and JSON Developed front end and backend modules of the project using Python with the Django Framework Designed and developed the data management systems using MySQL databases Written Python Scripts to parse the XML documents and load the data in the database Utilized the existing Python modules and rewritten to deliver data in required formats Worked on clientside validations and manipulations using HTML XML CSS and JQuery Written indexes views constraints stored procedures triggers cursors and user defined functions orsubroutines in MySQL Responsible for debugging and troubleshooting the application Utilized a Subversion control tool to coordinate teamwork Used Selenium Libraries to write a fully functioning test automation processEnvironment Python NumPy SciPy Dockers Pandas C JSON Oracle DB MySQL DB2 PLSQL Linux HTMLXHTML CSS AJAX JavaScript Apache Web Server\n",
      "\n",
      "Data extraction and structuring complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import re\n",
    "import os\n",
    "from docx import Document  # Import for handling Word documents\n",
    "import nltk  # Ensure you have NLTK installed for sentence tokenization\n",
    "\n",
    "# Function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    full_text = []\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                # Remove unwanted punctuation including specific symbols and unwanted characters\n",
    "                cleaned_text = re.sub(r'[●/\\\\\\xa0]', '', text)  # Remove specific punctuation\n",
    "                cleaned_text = re.sub(r'[\\n\\t(){}:,\"\\'.,]', '', cleaned_text)  # Remove other punctuation\n",
    "                cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)  # Remove emojis and any other non-word characters\n",
    "                full_text.append(cleaned_text)\n",
    "    return ' '.join(full_text)  # Return the full text as a single string\n",
    "\n",
    "# Function to extract text from a Word document\n",
    "def extract_text_from_word(doc_path):\n",
    "    full_text = []\n",
    "    doc = Document(doc_path)\n",
    "    \n",
    "    for para in doc.paragraphs:\n",
    "        line = re.sub(r'[●/\\\\\\xa0]', '', para.text)  # Remove specific punctuation\n",
    "        line = re.sub(r'[\\n\\t(){}:,\"\\'.,]', '', line)  # Remove other punctuation\n",
    "        line = re.sub(r'[^\\w\\s]', '', line)  # Remove emojis and any other non-word characters\n",
    "        full_text.append(line)\n",
    "    \n",
    "    return ' '.join(full_text)  # Return the full text as a single string\n",
    "\n",
    "# Function to create chunks based on sentences\n",
    "def create_chunks(text, max_chunk_length=100):\n",
    "    sentences = nltk.sent_tokenize(text)  # Tokenize text into sentences\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_chunk_length:\n",
    "            current_chunk += \" \" + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())  # Append the current chunk\n",
    "            current_chunk = sentence  # Start a new chunk with the current sentence\n",
    "\n",
    "    if current_chunk:  # Append the last chunk if any\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Specify the folder path containing the resumes\n",
    "folder_path = r\"C:/mvi/New folder (4)/resume 2\"\n",
    "\n",
    "# Initialize a list to hold all extracted data\n",
    "all_data = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    resume_data = {\n",
    "        \"filename\": filename,\n",
    "        \"content\": \"\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if filename.endswith('.pdf'):\n",
    "        print(f\"Processing PDF: {filename}\")\n",
    "        pdf_text = extract_text_from_pdf(file_path)\n",
    "        resume_data[\"content\"] = pdf_text\n",
    "    elif filename.endswith('.docx'):\n",
    "        print(f\"Processing Word document: {filename}\")\n",
    "        word_text = extract_text_from_word(file_path)\n",
    "        resume_data[\"content\"] = word_text\n",
    "    \n",
    "    all_data.append(resume_data)\n",
    "\n",
    "# Process each resume and create chunks\n",
    "for resume in all_data:\n",
    "    print(f\"Filename: {resume['filename']}\")\n",
    "    chunks = create_chunks(resume['content'])\n",
    "    for chunk in chunks:\n",
    "        print(f\"Chunk: {chunk}\\n\")\n",
    "        \n",
    "print(\"Data extraction and structuring complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844aab69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8e3a5fad",
   "metadata": {},
   "source": [
    "print(all_data) #RAW NBConvert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e4dbcb",
   "metadata": {},
   "source": [
    "# connect to python pymongo atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f5f32a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in c:\\users\\leowa\\anaconda3\\lib\\site-packages (4.10.1)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\leowa\\anaconda3\\lib\\site-packages (from pymongo) (2.7.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\leowa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\leowa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\leowa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\leowa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\leowa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\leowa\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cdaba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e2e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb+srv://unstructured:unstructured@cluster0.01ejx.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2aec6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client['db2']\n",
    "collection = db['unstructured']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7e93fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'filename_1_chunk_1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates from the collection\n",
    "pipeline = [\n",
    "    {\"$group\": {\n",
    "        \"_id\": {\"filename\": \"$filename\", \"chunk\": \"$chunk\"},\n",
    "        \"count\": {\"$sum\": 1}\n",
    "    }},\n",
    "    {\"$match\": {\"count\": {\"$gt\": 1}}}\n",
    "]\n",
    "\n",
    "duplicates = collection.aggregate(pipeline)\n",
    "\n",
    "for doc in duplicates:\n",
    "    # Remove all duplicates except one\n",
    "    collection.delete_many({\"filename\": doc[\"_id\"][\"filename\"], \"chunk\": doc[\"_id\"][\"chunk\"]})\n",
    "\n",
    "# Now, create the unique index\n",
    "collection.create_index([(\"filename\", 1), (\"chunk\", 1)], unique=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5856d2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: Aarthi Data Engineer.docx\n",
      "Skipping empty chunk for Aarthi Data Engineer.docx\n",
      "\n",
      "Error inserting chunk: E11000 duplicate key error collection: db2.unstructured index: filename_1_chunk_1 dup key: { filename: \"Aarthi Data Engineer.docx\", chunk: \"AARTHI AWASTHI aarthi211097gmailcom 316 530 1525 SUMMARY Over 7 years of IT experience in Design Development Maintenance and Support of Big Data Appli...\" }, full error: {'index': 0, 'code': 11000, 'errmsg': 'E11000 duplicate key error collection: db2.unstructured index: filename_1_chunk_1 dup key: { filename: \"Aarthi Data Engineer.docx\", chunk: \"AARTHI AWASTHI aarthi211097gmailcom 316 530 1525 SUMMARY Over 7 years of IT experience in Design Development Maintenance and Support of Big Data Appli...\" }', 'keyPattern': {'filename': 1, 'chunk': 1}, 'keyValue': {'filename': 'Aarthi Data Engineer.docx', 'chunk': 'AARTHI AWASTHI aarthi211097gmailcom 316 530 1525 SUMMARY Over 7 years of IT experience in Design Development Maintenance and Support of Big Data Applications Experience in Data Engineering Data Pipeline Design Development and Implementation as a Data EngineerData Developer and Data Modeler Optimized data queries and data processing tasks within Azure Synapse Analytics and Azure Data Factory improving performance and efficiency Experience on Migrating SQL database to Azure Data Lake Azure Data Lake Analytics Azure SQL Database Azure Data Bricks and Azure SQL Data Warehouse and controlling and granting database access and Migrating Onpremises databases to Azure Data Lake store using Azure Data Factory Strong experience in Software Development Life Cycle SDLC including Requirements Analysis Design Specification and Testing as per Cycle in both Waterfall and Agile methodologies Strong experience in writing scripts using Python API PySpark API and Spark API for analyzing data Python Libraries PySpark Pytest Pymongo PyExcel Psycopg NumPy and Pandas Hands On experience on Spark Core Spark SQL Spark Streaming and creating Data Frames handle in SPARK with Scala Experience in NoSQL databases and worked on table row key design and to load and retrieve data for realtime data processing and performance improvements based on data access patterns Extensive experience in Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MapReduce concepts Expertise in LLM models with ChatGPT and OpenAI Experience in building largescale highly available Web Applications Working knowledge of web services and other integration patterns Developed Simple to complex MapReduce and Streaming jobs using Java and Scala language Developed Hive scripts for end useranalyst requirements to perform ad hoc analysis with Hive to handle less important bulk ETL jobs Handson use of Spark and Scala API to compare the performance of Spark with Hive and SQL and Spark SQL to manipulate Data Frames in Scala Expertise in Python and Scala userdefined functions UDF for Hive and Pig using Python Experience in developing MapReduce Programs using Apache Hadoop for analyzing the big data as per the requirement Handson Spark MLlib utilities such as classification regression clustering collaborative filtering dimensionality reduction Handson experience in developing and deploying enterprisebased applications using major Hadoop ecosystem components like MapReduce YARN Hive HBase Flume Sqoop Spark MLlib Spark GraphX Spark SQL Kafka Experience in application of various data sources like Oracle SE2 SQL Server Flat Files and Unstructured files into a data warehouse Able to use Sqoop to migrate data between RDBMS NoSQL databases and HDFS Experience in Extraction Transformation and Loading ETL data from various sources into Data Warehouses as well as data processing like collecting aggregating and moving data from various sources using Apache Flume Kafka PowerBI and Microsoft SSIS Skilled in System Analysis ERDimensional Data Modeling Database Design and implementing RDBMS specific features Knowledge of working with Proof of Concepts PoCs and gap analysis and gathered necessary data for analysis from different sources prepared data for data exploration using data munging and Teradata Well experienced in Normalization and Denormalization techniques for optimum performance in relational and dimensional database environments Worked extensively with integrating Druid into big data pipelines leveraging tools like Apache Kafka and Hadoop for seamless data flow and transformation Focused on optimizing Druid cluster configurations tuning indexing and segment loading to improve query performance and data retrieval times Experience includes building interactive dashboards that leverage Druids fast querying capabilities providing realtime insights and reporting for businesscritical applications Experience in developing customized UDFs in Python to extend Hive and Pig Latin functionality Good working experience on AWS infrastructure services Amazon S3 EMR Lambda functions and Amazon EC2 Understanding of snowflake cloud technology with snowflake utilities Snow SQL Snow pipe and bigdata model  TECHNICAL SKILLS Data Modeling Tools Erwin Data Modeler ER Studio v17 Programming Languages SQL PLSQL UNIX Mongo DB Data wave Python R SQL Server PostgreSQL database Methodologies RAD JAD System Development Life Cycle SDLC Agile Cloud Platform AWS Azure cloud Google Cloud Azure data factory Databases Oracle 12c11g Teradata R15R14 OLAP Tools Tableau SSAS SSIS Business Objects and Crystal Reports 9 ETLData warehouse Tools Informatica 9691 Tableau Splunk IntelliJ and New Relic Data Visualization Tools Tableau Power BI SAS Excel ETL   Operating System Windows Unix Sun Solaris Mac OS Big Data Tools Hadoop Ecosystem Map Reduce  EDUCATION Bachelors in commerce from Osmania university 2015  EXPERIENCE Walmart AR   Sr Data Engineer                                                                                                                             May 2023 to Present Responsibilities  Analyze and cleanse raw data using HiveQL Proficient in using Azure services such as Azure Data Lake Azure SQL Database Azure Synapse Analytics and Azure Cosmos DB for scalable data storage processing and analytics Experience in data transformations using MapReduce Hive for different file formats Analyze the user needs interact with various SORs to understand their incoming data structure and run POCs with the best possible processing framework in big data platform Worked with Spark for improving performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Utilized Microsoft Azure services such as Azure Data Factory Azure Blob Storage and Azure SQL Database to support data operations and analytics Developed UNIX shell scripts to load large number of files into HDFS from Linux File System Played a key role in Finalizing the tech stack for our project GPC and ran endtoend vigorous testing qualifying the user needs as well as tech requirements Experience with Apache Druid utilizing its capabilities to ingest and analyze large volumes of event data in realtime Developed and maintained scalable architectures using Druid for handling large datasets with high query performance ensuring efficient resource management and lowlatency querying Ran data formatting scripts in Python and created terabyte CSV files to be consumed by Hadoop MapReduce jobs Expertise in Snowflake to create and Maintain Tables and views Performed Kafka analysis feature selection feature extraction using Apache Spark Machine Learning streaming libraries in Python Skilled in creating and managing tables views indexes and stored procedures in SQL Server Used Pandas Numpy Scipy Scikitlearn NLTK in Python for scientific computing and data analysis Experienced in using Pandas Numpy SciPy Scikitlearn to develop various machine learning algorithms Developed Python code using version control tools like GIT and SVN on Vagrant machines Collaborated with intra applications teams to fit our business models on existing onprem platform setup Experience in creating tables dropping and altering at runtime without blocking updates and queries using HBase and Hive Worked on importing and exporting data from Snowflake Oracle and DB2 into HDFS and Hive using Sqoop for analysis visualization and to generate reports Encoded and decoded JSON objects using PySpark to create and modify the data frames in Apache Spark Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Experienced with popular NLP libraries such as NLTK SpaCy Hugging Face Transformers and Gensim leveraging them for various text processing and analysis tasks  Designed and implemented scalable ETL pipelines using Azure Data Factory integrating data from various sources and ensuring smooth data flow across Walmarts systems Strong PySpark experience essential for developing and optimizing data pipelines and transformations Extensive Python experience showcasing proficiency in core Python programming and scripting Demonstrated expertise in using Azure Databricks for data engineering tasks Handson experience with Azure Data Factory and Azure Data Lake for orchestrating and managing data workflows Proven ability to build reusable frameworks libraries and data platform capabilities Experience with REST API and GraphQL APIs highlighting knowledge of API integration and management Experience with Data Quality DQ capabilities focusing on building reusable DQ frameworks Ability to work independently while demonstrating system design thinking Used CloudWatch to monitor logs and log metrics generated by applications Integrated with Restful APIs to create ServiceNow Incidents when there is a process failure within the batch job Analyzed the SQL scripts and designed the solution to implement using PySpark Developed a capability to implement audit logging at required stages while applying business logic Implemented Spark Data Frames on huge incoming data sets of various data formats like JSON CSV Parquet Actively worked in resolving many of the Tech challenges One of them is handling the nested JSON with multiple data sections in the same file and converting them into Spark friendly data frames Reformatted the end results to SORs requested formats Environment Spark AWS Python Pandas sy HiveQL MySQL SOAP Snowflake NIFI Cassandra Spark SQL PySpark Cloudera HDFS Hive Apache Kafka Sqoop Scala Shell scripting Linux MySQL Oracle Enterprise DB Flink Modern data Five Tran Jenkins Eclipse Oracle Git  Charter Communications Negaunee MI                                                                                   Aug 2021 to Dec 2022 Data Engineer Responsibilities  Performed data extraction transformation loading and integration in data warehouse operational data stores and master data management Developed Spark Applications by using Scala and Implemented Apache Spark data processing project to handle data from various RDBMS and Streaming sources Data ingestion to one or more Azure services Azure Data Lake Azure Storage Azure SQL Azure DW and processing the data in Azure Databricks Implemented Apache Airflow for authoring scheduling and monitoring Data Pipelines Developed Mappings using Transformations like Expression Filter Joiner and Lookups for better data messaging and to migrate clean and consistent data Worked on setting up high availability for major production clusters and designed automatic failover control using Zookeeper and quorum journal nodes Designed several DAGs Directed Acyclic Graph for automating ETL pipelines Provide troubleshooting and best practices methodology for development teams Produce unit tests for Spark transformations and helper methods Design data processing pipelines Multiple batch jobs were written for processing hourly and daily data received through multiple sources like Adobe NoSQL databases Leveraged cloud and Azure computing technologies for automated machine learning and analytics pipelines Designed and implemented configurable data delivery pipeline for scheduled updates to customer facing data stores built with Python Involved in building the ETL source to Target mapping to load data into Data warehouse Developed a batch data ingestion pipeline using Sqoop and Hive to ingest transform and analyze Supply Chain data Use ObjectOriented Programming concepts to build Hive UDFs in Python that could be reused across the pipeline Extensively working on Hive queries to load data from various sources like Teradata DB2 Oracle Mainframe etc Extensively working on developing YAML files which are passed to the internal framework scripts having all the required JARs to move data Migrating the transformed data to Azure data lake to be consumed by consumers depending on the business need Creating tables in Hive in ORC format using correct compression ensuring no data loss Worked on performance tuning of the queries costbased optimizations to improve the performance and latency of the queries Writing shell scripts to automate the migration process and validate the data after loading Created data validation application in Scala for validating the data of source and target Working on performance tuning of the code with help of Hive dynamic partitioning set parameters mapper and reducer tuning Coordinate with Quality Assurance teams to ensure the developed code is accurately transformed into a working product Participate in daily meetings and standups to provide updates and project progress to the team Analyzed the system for new enhancementsfunctionalities and perform Impact analysis of the application for implementing ETL changes Implemented a Continuous Delivery pipeline with Docker and GitHub Azure Built performant scalable ETL processes to load cleanse and validate data Preparing associated documentation for specifications requirements and testing Environment Azure Data Factory USQL Azure Data Lake Analytics Azure SQL Azure DW Databricks GitHub Docker Talend Big Data Integration Snowflake Oracle SQL Server MySQL NoSQL MongoDB HBase Cassandra PythonPySpark Pytest Pymongo PyExcel Psycopg Matplotlib NumPy and Pandas  IBing Software Solutions Private Limited Hyderabad India                                                      Sep 2018 to Jul 2021 Data Engineer Responsibilities  Gather business requirements definition and design of the data sourcing and worked with the data warehouse architect on the development of logical data models Created sophisticated visualizations calculated columns and custom expressions and developed Map Chart Cross table Bar chart Tree map and complex reports which involves Property Controls Custom Expressions Investigated market sizing competitive analysis and positioning for product feasibility Worked on Business forecasting segmentation analysis and Data mining Extensively used Agile methodology as the Organization Standard to implement the data Models Used Micro service architecturebased services interacting through a combination of REST and Apache Kafka message brokers Created several types of data visualizations using Python  Matplotlib and Tableau Extracted data using SQL Queries to create reports Performed reverse engineering using Erwin to redefine entities attributes and relationships existing database Analyze functional and nonfunctional business requirements and translate into technical data requirements and create or update existing logical and physical data models Developed a data pipeline using Kafka to store data into HDFS Performed Regression testing for Golden Test Cases from State end to end test cases and automated the process using Python scripts Developed Spark jobs using Scala for faster realtime analytics and used Spark SQL for querying Generated graphs and reports using the ggplot package in RStudio for analytical models Developed and implemented R and Shiny application which showcases machine learning for business forecasting Developed predictive models using Decision Tree Random Forest and Naïve Bayes Used pandas NumPy seaborn SciPy Matplotlib Scikitlearn NLTK in Python for developing various machine learning algorithms Expertise in R Matlab Python and respective libraries Research on Reinforcement Learning and control TensorFlow Torch and machine learning model Scikitlearn Performed Kmeans clustering Regression and Decision Trees in R Worked on data cleaning and reshaping generated segmented subsets using NumPy and Pandas in Python Implemented various statistical techniques to manipulate the data like missing data imputation principal component analysis and sampling Responsible for design and development of Python programsscripts to prepare transform and harmonize data sets in preparation for modeling Worked with Market Mix Modeling to strategize the advertisement investments to better balance the ROI on advertisements Implemented clustering techniques like DBSCAN Kmeans Kmeans and Hierarchical clustering for customer profiling to design insurance plans according to their behavior pattern Used Grid Search to evaluate the best hyperparameters for my model and Kfold cross validation technique to train my model for best results Used Python 3X NumPy SciPy pandas scikitlearn seaborn and Spark 20 PySpark MLlib to develop a variety of models and algorithms for analytic purposes Performed Data Cleaning features scaling features engineering using pandas and NumPy packages in Python and build models using deep learning frameworks Implemented Univariate Bivariate and Multivariate Analysis on the cleaned data for getting actionable insights on the 500product sales data by using visualization techniques in Matplotlib Seaborn Bokeh and created reports in Power BI Processed the image data through the Hadoop distributed system by using Map and Reduce then stored into HDFS Created Session Beans and controller Servlets for handling HTTP requests from Talend Performed Data Visualization and Designed Dashboards with Tableau and generated complex reports including charts summaries and graphs to interpret the findings to the team and stakeholders Wrote documentation for each report including purpose data source column mapping transformation and user group Utilized Waterfall methodology for team and project management Used Git for version control with the Data Engineer team and Data Scientists colleagues Environment Spark YARN HIVE Pig Scala Mahout NiFi TDD Python Hadoop Azure DynamoDB Kibana NoSQL Sqoop MySQL  Maisa Solutions Private Limited Hyderabad India                                                                   Jun 2016 to Aug 2018 Hadoop developer Responsibilities Requirement discussions design the solution Responsible for choosing the Hadoop components Hive Pig MapReduce Sqoop Flume etc Responsible for building scalable distributed data solutions using Hadoop Hadoop cluster building and ingestion of data using Sqoop Imported streaming logs to HDFS through Flume Used Flume to collect aggregate and store the web log data from different sources like web servers mobile and network devices and pushed to HDFS Developed Use cases and technical prototyping for implementing Hive and Pig Worked in analyzing data using Hive Pig and custom MapReduce programs in Java Implemented partitioning dynamic partitions and buckets in Hive Installed and configured Hive Sqoop Flume Oozie on the Hadoop cluster Involved in scheduling Oozie workflow engine to run multiple Hive and Pig jobs Tuned the Hadoop Clusters and Monitored for the memory management and for the MapReduce jobs Responsible for Cluster maintenance Adding and removing cluster nodes Cluster Monitoring and Troubleshooting Developed a custom Framework capable of solving small files problem in Hadoop Deployed and administered 70 node Hadoop clusters Administered two smaller clusters Environment MapReduce HBase HDFS Hive Pig Java SQL Cloudera Manager Sqoop Flume Oozie Java JDK 16 Eclipse'}}\n",
      "\n",
      "Filename: Avinash Kormatha.pdf\n",
      "Skipping empty chunk for Avinash Kormatha.pdf\n",
      "\n",
      "Error inserting chunk: E11000 duplicate key error collection: db2.unstructured index: filename_1_chunk_1 dup key: { filename: \"Avinash Kormatha.pdf\", chunk: \"Avinash KormathaPhone 512 8511476Email avinashkormathagmailcomPROFESSIONAL SUMMARY Over 6 years of experience in design development and analysis of Py...\" }, full error: {'index': 0, 'code': 11000, 'errmsg': 'E11000 duplicate key error collection: db2.unstructured index: filename_1_chunk_1 dup key: { filename: \"Avinash Kormatha.pdf\", chunk: \"Avinash KormathaPhone 512 8511476Email avinashkormathagmailcomPROFESSIONAL SUMMARY Over 6 years of experience in design development and analysis of Py...\" }', 'keyPattern': {'filename': 1, 'chunk': 1}, 'keyValue': {'filename': 'Avinash Kormatha.pdf', 'chunk': 'Avinash KormathaPhone 512 8511476Email avinashkormathagmailcomPROFESSIONAL SUMMARY Over 6 years of experience in design development and analysis of Python Django and clientservertechnologiesbased applications Good experience in various phases of SDLC Requirement Analysis Design Development and Testing onvarious Development and Enhancement Projects Hands on experience in Agile Methodologies Scrum stories and sprints experience in a Python basedenvironment Experience in Object Oriented Design and Programming concepts using Python 3x and Java Experience working on several Standard Python Packages like NumPy Pandas Matplotlib PySide SciPywxPython PyTables etc Knowledge about setting up Python REST API Framework using Django Experience working with Python ORM Libraries including Django ORM Experience in implementing Model View Control MVC architecture using serverside applications likeDjango Flask and Pyramid for developing web applications Good experience in error and exceptional handling Strong experience in designing the automation framework using Shell scripting Experience in writing test plans test cases test specifications and test coverage Good experience in Testing framework by adding some helper classes and methods Good knowledge in implementation of Python best Practices PEP8 Experience in designing the automation framework using Perl and Shell scripting Experience in developed tested and documented initial release of Honeycombs Open Telemetry distributionfor Java while embedded in the integrations team Extensive experience in creating the Automation scripts using Python for testing applications as well as theintegration of these application APIs UIs based on REST calls using Python phrasing the JSON responses Experience in HTMLHTML5 DHTML CSSCSS3 JavaScript XML JSON Oracle PLSQL and Postgres Experience in writing Subqueries Stored Procedures Triggers Cursors and Functions on MySQL andPostgreSQL database Expertise in performing research to explore and identify new technological platforms Expertise in evangelizing open telemetrydistributed tracing both internally and externally Experience in creating Data Layer in MySQL Experience on extracted and loaded data using Python scripts and PLSQL packages Experience in migrating applications to the AWS cloud and involved in DevOps processes for build and deploysystemsTECHNICAL SKILLSLanguages PythonFrameworks DjangoPython Packages NumPy Pandas Matplotlib PySide SciPy wxPython PyTablesCloud Services AWS VMware Microsoft AzureWeb Technologies CSS HTML Bootstrap jQuery JavaScript Angular JSONWebApp Server Nginx Apache Tomcat IISIDE PyCharm PLSQL Developer TOADOther Tools Jenkins IBM Integration and Web Builder JIRA Bugzilla Rally Version OneManagement Tools SVN GitDatabases Oracle 1011g MySQL SQL ServerOperating Systems Unix Linux Windows and Mac OSEDUCATION  Master of Science in CS University of Texas at Arlington Arlington TX Bachelor of Technology in CSE GITAM University Hyderabad IndiaPROFESSIONAL EXPERIENCE Kroger Blue Ash OH Mar 2023  PresentPython Developer Evaluating business requirements and preparing detailed specifications that follow project guidelinesrequired to develop written programs Working on various phases of Software Development Life Cycle using Agile  Scrum Software developmentmethodology Fixing bugs enhancing applications by improving code reuse and upgraded performance by making effectiveuse of various design patterns Developed tested and documented initial release of Honeycombs Open Telemetry distribution for Javawhile embedded in the integrations team Evangelizing open telemetrydistributed tracing both internally and externally Building a distributed system for triggering and executing daily data processing jobs which contains a highavailability scheduler built with Python a cluster of workers built with Python and UI built with Pythonand Django Designed and developed highly scable distributed tracing pipeline leveraging open source opentelemetryspecification Responsible for the development of entire backend modules using Python Integrating Amazon Cloud Watch with Amazon EC2 instances for monitoring the log files and track metrics Creating AWS S3 buckets performed folder management in each bucket managing cloud trail logs andobjects within each bucket Used Apache Couchdb NoSQL in AWS Linux instance in parallel to RDS MySQL to store and analyze jobmarket info Creating Highly Available Environments using AutoScaling Load Balancers and SQS Automating the existing scripts for performance calculations using NumPy and SQL alchemy Defining branching labelling and merge strategies for all applications in Git Configuring Elastic Load Balancers with EC2 Auto Scaling groups Configuring S3 to host Static Web content Responsible for S3 Versioning and lifecycle policies to and backup files and archive files in Glacier Creating monitors alarms and notifications for EC2 hosts using CloudWatch Working on Performance Tuning and Query Optimization in AWS Redshift Designing application on AWS taking advantage of Disaster recovery Developing Cloud Formation scripts to build on demand EC2 instance formation Utilizing AWS CLI to automate backups of ephemeral datastores to S3 buckets and EBS Creating nightly AMIs for mission critical production servers as backups Configuring and maintaining the monitoring and alerting of production and corporate serversstorage usingCloudWatchEnvironment Python Django NumPy HTMLCSS MySQL GIT CICD GitHub AWS RDS IAM S3 Cloud WatchJPMC Plano TX Mar 2022  Feb 2023Python Developer Developed application logic using Python JavaScript Java Designed front end and backend of the application utilizing Python on Django Web Framework Configured AWS Multi Factor Authentication in IAM to implement 2 step authentication of users access usingGoogle Authenticator and AWS Virtual MFA Included security groups network ACLs Internet Gateways and Elastic IPs to ensure a safe area fororganization in AWS public cloud Used Python modules like Restful Matplotlib and Pandas library for statistical analysis and generatingcomplex graphical data and NumPy for numerical analysis  Written UNIX shell scripts to automate the jobs and scheduling Cron jobs for job automation using commandswith Crontab Wrote Ansible Playbooks with Python SSH as the Wrapper to Manage Configurations of AWS Nodes and TestPlaybooks on AWS instances using Python Designed AWS Cloud Formation templates to create custom sized VPC subnets NAT to ensure successfuldeployment of Web applications and database templates Created scripts in Python which integrated with Amazon API to control instance operations Coordinatedassisted developers with establishing and applying appropriate branching labelling namingconventions using GIT source control Developed Merge jobs in Python to extract and load data into MySQL database Built numerous Lambda functions using python and automated the process using the event created Involved in designing and developing Amazon EC2 Amazon S3 Amazon RDS Amazon Elastic Load BalancingAmazon SQS and other services of the AWS infrastructure Ran build jobs and integration tests on Jenkins MasterSlave configuration Managed Servers on the Amazon Web Services AWS platform instances Built servers using AWS importing volumes launching EC2 RDS creating security groups autoscaling loadbalancers ELBs in the defined virtual private connection Maintained the reliability availability and performance of Amazon Elastic Compute Cloud Amazon EC2instances Good understanding of NoSQL databases such as MongoDB Datastax Redis and Apache Cassandra Worked as part of a team to design and develop cloud data solutions Conducted systems design feasibility and cost studies and recommend costeffective cloud solutions such asAmazon Web Services AWS Monitored the AWS resources using Cloud Watch and application resources using Nagios Involved in complete SDLC life cycle  Designing Coding Testing Debugging and Production Support Involved in daytoday maintenance and problem analysis on storageEnvironment Python Django Matplotlib Pandas MySQL Linux HTML XHTML SVN CSS AJAX BugzillaJavaScript Apache Web ServerAmerican Automobile Association Phoenix AZ Oct 2020  Dec 2021Python Developer Responsible for gathering requirements system analysis design development testing and deployment Built DevOps Pipeline from the scratch using AWS Resources with Blue Green and Red Black Deploymentenabled Developed user interface using CSS HTML and ReactJS with Redux framework Created reports and dashboards by using Tableau 9x to communicate data insights significant featuresmodels score and the performance of new recommendation system to both technical and business teams Designed Forms Views Models using Djangos MVC software architecture pattern Used Python and Pandas library for data cleaning and aggregation Created RESTful APIs using Django Used Django Database APIs to successfully create database objects Used Ansible to configure and manage the infrastructure Wrote Python modules to extractload asset data from the MySQL source database Designed and developed UseCases Classes and Object Diagrams using UML Rational Rose for ObjectOriented Design techniques Designed and implemented a dedicated MySQL database server to drive the web apps and report on dailyprogress Used PyUnit the Python unit test framework for all Python applications Involved in developing the REST Web services to expose the business methods to external services in theproject Worked on front end frameworks like CSS Bootstrap for development of Web applications Participated in requirement gathering and worked closely with the architect in designing and modelling Developed tested and debugged software tools for clients and internal customers of the organization Coded test programs and evaluated existing engineering processes Created a Git repository and added the project to GitHub Environment Python Flask MongoDB JSON jQuery Elastic search XML PyMongo AWSKPIT Cummins India July 2019  Sep 2020Python Developer Implemented ETL as a code solution with Python using modules like SQL Alchemy and automated adminactivities to be taken care of from schemarelated changes to be taken care of from the deployment pipeline Responsible for the development of entire frontend and backend modules using Python Developed RESTful services using Django Wrote Python routines to log into the websites and fetch data for selected options Performed testing using Djangos Test Module Used Pandas NumPy Matplotlib SciPy in Python Designed and developed ETL integration patterns using Python on Spark Worked on specification of platforms interfaces and development tools Used Python scripts for automation of production tasks Generated property list for every application dynamically using Python Rewrote existing Java application in Python module to deliver a certain format of data Analysed various crossfunctional multiplatform applications systems enforcing Python best practices andprovided guidance in making long term scalable architectural design decisions Involved in various phases of Software Development Life Cycle SDLC such as requirements gatheringmodelling analysis design and development Used PyQuery for selecting DOM elements when parsing HTML Created PyUnit test scripts and used them for unit testing Worked with JSON based REST Web servicesEnvironment Python Django Pandas NumPy Matplotlib SciPy SQL Alchemy Spark PyQuery PyUnit RestfulJSON SDLCUnilever India May 2018  Jun 2019Python Developer Worked on development of application architecture and blueprints to define application components Designed and developed the user interface of the project with HTML CSS and JSON Developed front end and backend modules of the project using Python with the Django Framework Designed and developed the data management systems using MySQL databases Written Python Scripts to parse the XML documents and load the data in the database Utilized the existing Python modules and rewritten to deliver data in required formats Worked on clientside validations and manipulations using HTML XML CSS and JQuery Written indexes views constraints stored procedures triggers cursors and user defined functions orsubroutines in MySQL Responsible for debugging and troubleshooting the application Utilized a Subversion control tool to coordinate teamwork Used Selenium Libraries to write a fully functioning test automation processEnvironment Python NumPy SciPy Dockers Pandas C JSON Oracle DB MySQL DB2 PLSQL Linux HTMLXHTML CSS AJAX JavaScript Apache Web Server'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each resume and create chunks, then upload to MongoDB\n",
    "for resume in all_data:\n",
    "    print(f\"Filename: {resume['filename']}\")\n",
    "    chunks = create_chunks(resume['content'])\n",
    "    for chunk in chunks:\n",
    "        if not chunk.strip():  # Skip empty chunks\n",
    "            print(f\"Skipping empty chunk for {resume['filename']}\\n\")\n",
    "            continue\n",
    "        \n",
    "        document = {\n",
    "            \"filename\": resume['filename'],\n",
    "            \"chunk\": chunk\n",
    "        }\n",
    "        try:\n",
    "            # Insert the document into the collection\n",
    "            collection.insert_one(document)\n",
    "            print(f\"Inserted chunk into MongoDB: {chunk}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting chunk: {e}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d6c612",
   "metadata": {},
   "source": [
    "# download the output csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ea51c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='all_data.csv' target='_blank'>all_data.csv</a><br>"
      ],
      "text/plain": [
       "C:\\Users\\leowa\\all_data.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "all_data.append(resume_data)\n",
    "\n",
    "# Convert all_data to a DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = \"all_data.csv\"\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Create a download link\n",
    "display(FileLink(csv_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29f7e71",
   "metadata": {},
   "source": [
    "# download the output text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8689a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='all_data.txt' target='_blank'>all_data.txt</a><br>"
      ],
      "text/plain": [
       "C:\\Users\\leowa\\all_data.txt"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " all_data.append(resume_data)\n",
    "\n",
    "# Save all_data to a text file\n",
    "text_file_path = \"all_data.txt\"\n",
    "with open(text_file_path, 'w', encoding='utf-8') as f:\n",
    "    for resume in all_data:\n",
    "        f.write(f\"Filename: {resume['filename']}\\n\")\n",
    "        f.write(f\"Content: {resume['content']}\\n\\n\")\n",
    "\n",
    "# Create a download link\n",
    "display(FileLink(text_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d93f13f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3bb833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
